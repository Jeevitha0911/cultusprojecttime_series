# ============================================================
# Advanced Time Series Forecasting with Uncertainty Quantification
# ============================================================

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# ----------------------------
# CONFIG
# ----------------------------
SEQ_LEN = 48
BATCH_SIZE = 64
EPOCHS = 20
LR = 0.001
QUANTILES = [0.1, 0.5, 0.9]
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

torch.manual_seed(42)
np.random.seed(42)

# ----------------------------
# 1. DATA GENERATION
# ----------------------------

def generate_data(n=5000):
    t = np.arange(n)
    s1 = 10 + np.sin(2 * np.pi * t / 24) + 0.3 * np.random.randn(n)
    s2 = 5 + np.cos(2 * np.pi * t / 168) + 0.3 * np.random.randn(n)
    s3 = 0.01 * t + np.random.randn(n)
    return pd.DataFrame({"x1": s1, "x2": s2, "x3": s3})

df = generate_data()

# Differencing
df = df.diff().dropna()

# Scaling
scaler = MinMaxScaler()
scaled = scaler.fit_transform(df)

# ----------------------------
# 2. DATASET
# ----------------------------

class TimeDataset(Dataset):
    def __init__(self, data, seq_len):
        self.data = data
        self.seq_len = seq_len

    def __len__(self):
        return len(self.data) - self.seq_len

    def __getitem__(self, idx):
        x = self.data[idx:idx + self.seq_len]
        y = self.data[idx + self.seq_len, 0]  # predict x1
        return (
            torch.tensor(x, dtype=torch.float32),
            torch.tensor(y, dtype=torch.float32),
        )

dataset = TimeDataset(scaled, SEQ_LEN)

train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size

train_ds, test_ds = torch.utils.data.random_split(
    dataset, [train_size, test_size]
)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)

# ----------------------------
# 3. PINBALL LOSS
# ----------------------------

class PinballLoss(nn.Module):
    def __init__(self, quantiles):
        super().__init__()
        self.quantiles = quantiles

    def forward(self, preds, target):
        target = target.unsqueeze(1)
        losses = []

        for i, q in enumerate(self.quantiles):
            err = target[:, 0] - preds[:, i]
            loss = torch.max(q * err, (q - 1) * err)
            losses.append(loss.unsqueeze(1))

        return torch.mean(torch.cat(losses, dim=1))

# ----------------------------
# 4. MODEL
# ----------------------------

class LSTMQuantile(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, len(QUANTILES))

    def forward(self, x):
        out, _ = self.lstm(x)
        last = out[:, -1, :]
        return self.fc(last)

model = LSTMQuantile(input_size=3, hidden_size=64).to(DEVICE)

optimizer = torch.optim.Adam(model.parameters(), lr=LR)
criterion = PinballLoss(QUANTILES)

# ----------------------------
# 5. TRAINING
# ----------------------------

for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0.0

    for x, y in train_loader:
        x = x.to(DEVICE)
        y = y.to(DEVICE)

        optimizer.zero_grad()
        preds = model(x)
        loss = criterion(preds, y)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    print(
        f"Epoch {epoch + 1}/{EPOCHS} "
        f"Loss: {epoch_loss / len(train_loader):.4f}"
    )

# ----------------------------
# 6. EVALUATION
# ----------------------------

def evaluate(model, loader):
    model.eval()
    preds, trues = [], []

    with torch.no_grad():
        for x, y in loader:
            x = x.to(DEVICE)
            p = model(x).cpu().numpy()
            preds.append(p)
            trues.append(y.numpy())

    return np.vstack(preds), np.concatenate(trues)

P, Y = evaluate(model, test_loader)

median_pred = P[:, 1]
rmse = np.sqrt(mean_squared_error(Y, median_pred))
mae = mean_absolute_error(Y, median_pred)

# Pinball scores
pinball_scores = []
for i, q in enumerate(QUANTILES):
    err = Y - P[:, i]
    pinball_scores.append(np.mean(np.maximum(q * err, (q - 1) * err)))

# Coverage
lower = P[:, 0]
upper = P[:, 2]
coverage = np.mean((Y >= lower) & (Y <= upper))

print("\n---- Neural Network Metrics ----")
print("RMSE:", rmse)
print("MAE:", mae)
for q, s in zip(QUANTILES, pinball_scores):
    print(f"Pinball(q={q}):", s)
print("Coverage:", coverage)

# ----------------------------
# 7. BASELINE MODEL
# ----------------------------

series = df["x1"].values

train_series = series[: train_size]
test_series = series[train_size : train_size + test_size]

hw = ExponentialSmoothing(
    train_series, trend="add", seasonal=None
).fit()

baseline_pred = hw.forecast(len(test_series))

b_rmse = np.sqrt(mean_squared_error(test_series, baseline_pred))
b_mae = mean_absolute_error(test_series, baseline_pred)

print("\n---- Baseline (Exponential Smoothing) ----")
print("RMSE:", b_rmse)
print("MAE:", b_mae)

# ----------------------------
# 8. CALIBRATION CHECK
# ----------------------------

bins = np.linspace(0.1, 0.9, 9)
obs = []

for b in bins:
    q_pred = np.quantile(P[:, 1], b)
    obs.append(np.mean(Y <= q_pred))

print("\nCalibration (Expected vs Observed)")
for b, o in zip(bins, obs):
    print(round(b, 2), "->", round(o, 2))

print("\nFinished Successfully âœ…")
