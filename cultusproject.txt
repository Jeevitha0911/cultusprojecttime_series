Step-by-Step Description (Procedure)
Step 1: Import Required Libraries
The project begins by importing essential Python libraries:
NumPy and Pandas for numerical and data manipulation
PyTorch for deep learning model development
Scikit-learn for scaling and error metrics
Statsmodels for baseline statistical forecasting
These libraries provide the foundation for data processing, modeling, training, and evaluation.
Step 2: Configure Hyperparameters
Important parameters such as:
Sequence length
Batch size
Learning rate
Number of epochs
Quantile levels (0.1, 0.5, 0.9)
are defined at the beginning. This ensures reproducibility and easy tuning of the model.
Step 3: Generate Multivariate Time Series Data
A synthetic multivariate dataset is programmatically generated consisting of:
Feature 1: Daily sinusoidal seasonality
Feature 2: Weekly cosine seasonality
Feature 3: Linear trend with noise
This simulates a real-world dataset with trend, seasonality, and noise.
Step 4: Apply Differencing
First-order differencing is applied to remove trend and stabilize the mean of the time series. This improves stationarity and makes learning easier for the neural network.
Step 5: Normalize the Data
Min-Max normalization scales all features to the range [0,1]. This prevents features with large values from dominating training and improves convergence.
Step 6: Create Sliding Window Sequences
A custom PyTorch Dataset class converts the continuous time series into input-output pairs:
Input â†’ Last 48 time steps of all features
Output â†’ Next time step of the target variable
This transforms the data into supervised learning format.
Step 7: Train-Test Split
The dataset is split chronologically:
80% for training
20% for testing
This preserves temporal order and prevents future information leakage.
Step 8: Define Pinball (Quantile) Loss Function
A custom Pinball Loss function is implemented.
For each quantile, asymmetric penalties are applied:
Under-prediction â†’ weighted by quantile
Over-prediction â†’ weighted by (quantile âˆ’ 1)
This enables learning of conditional quantiles instead of only mean values.
Step 9: Build LSTM Quantile Regression Model
The model architecture consists of:
LSTM layer for capturing temporal dependencies
Fully connected layer producing three outputs (0.1, 0.5, 0.9 quantiles)
This allows the model to produce probabilistic forecasts.
Step 10: Model Training
For each epoch:
Input sequences are fed into the model
Quantile predictions are generated
Pinball loss is computed
Backpropagation updates model parameters
Training loss is printed after each epoch.
Step 11: Rolling-Origin Evaluation
The trained model is evaluated on unseen test data in a forward-moving manner, mimicking real forecasting conditions.
Predictions and actual values are stored for metric computation.
Step 12: Compute Point Forecast Metrics
Using the median (0.5 quantile) predictions:
Root Mean Squared Error (RMSE)
Mean Absolute Error (MAE)
are calculated to measure accuracy.
Step 13: Compute Probabilistic Metrics
For each quantile:
Pinball Score is computed
Additionally:
Coverage Rate = Percentage of actual values lying between 10% and 90% quantiles
These metrics evaluate uncertainty quality.
Step 14: Train Baseline Model
An Exponential Smoothing model is trained using the same training series.
Its RMSE and MAE are computed for comparison with the neural network.
Step 15: Calibration Analysis
Predicted median values are divided into probability bins.
For each bin:
Expected probability
Observed frequency of true values
are compared to assess how well predicted uncertainties align with reality.
Step 16: Display Final Results
All metrics including:
RMSE
MAE
Pinball Scores
Coverage Rate
Baseline metrics
Calibration results
are printed to the console.
ðŸ”¹ Model Architecture Description
The model uses a Long Short-Term Memory (LSTM) network followed by a fully connected output layer.
Input: 48 Ã— 3 sequence
Hidden units: 64
Output: Three neurons representing quantiles
The LSTM captures temporal patterns, while the output layer transforms latent representations into probabilistic forecasts.
ðŸ”¹ Hyperparameter Tuning Strategy
Learning rate selected as 0.001
Hidden units fixed at 64
Batch size = 64
Epochs = 20
Values were chosen based on commonly accepted best practices for time-series LSTM models and balanced training time with accuracy.
ðŸ”¹ Evaluation Strategy
Two evaluation perspectives are used:
Point Forecast Accuracy
RMSE, MAE
Probabilistic Accuracy
Pinball Score, Coverage Rate
Additionally, model performance is compared with a classical baseline.
ðŸ”¹ Calibration Interpretation
If observed probabilities closely match expected probabilities, the model is well calibrated.
Small deviations indicate acceptable uncertainty estimates
